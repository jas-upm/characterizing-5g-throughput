{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d39e3e",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c55ff565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import shap\n",
    "from xgboost import XGBRegressor\n",
    "from ngboost import NGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade5a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv('/v1_6_1_data_and_models/data_df.csv')\n",
    "original_df['timestamp'] = pd.to_datetime(original_df['timestamp'])\n",
    "original_df = original_df[original_df.carrier.isin(['CARRIER1', 'CARRIER2', 'CARRIER3'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af72de20",
   "metadata": {},
   "source": [
    "## Constant Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2bc881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune these\n",
    "folder_dir = '/v1_6_1_data_and_models/'\n",
    "\n",
    "verbose = True\n",
    "\n",
    "N_ESTIMATORS = 1000 # 1000\n",
    "PATIENCE_GB = 100 # 100\n",
    "LR_GB = 0.05\n",
    "\n",
    "RANDOM_SEED = 42 # 42\n",
    "\n",
    "technologies = ['4G', '5G_NSA', '5G_SA'] \n",
    "\n",
    "split_dates = {'train' : {'start' : '2024-05-01', \n",
    "                            'end' : '2025-08-18'},\n",
    "                'val' : {'start' : '2025-08-18',\n",
    "                        'end' : '2025-08-26'},\n",
    "                'test' : {'start' : '2025-08-26',\n",
    "                        'end' : '2025-09-10'}}\n",
    "\n",
    "# All features\n",
    "features = ['Latency', 'Jitter',\n",
    "       'Packet Loss', 'DL Throughput', 'DL TTFB', 'RSRP',\n",
    "       'RSRQ', 'SINR', 'Timing Advance',\n",
    "       'Frequency Band', 'Carrier']\n",
    "\n",
    "# Target variable\n",
    "target = 'DL Throughput'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62691fb1",
   "metadata": {},
   "source": [
    "## Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bf5734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(data_df, technology, features, split_dates):\n",
    "\n",
    "    data_df = data_df.loc[data_df['radio_access_technology'].isin([technology])]\n",
    "    data_df = data_df[['timestamp'] + features]\n",
    "    \n",
    "    # Totd Conversion\n",
    "    data_df['time_of_day'] = (data_df[\"timestamp\"].dt.hour \n",
    "                                + data_df[\"timestamp\"].dt.minute / 60\n",
    "                                + data_df[\"timestamp\"].dt.second / 3600)\n",
    "    data_df[\"weekday\"] = data_df.timestamp.dt.weekday.astype(int)\n",
    "\n",
    "    # Train val test split\n",
    "    train_df = data_df.loc[(data_df['timestamp'] >= split_dates['train']['start']) & (data_df['timestamp'] < split_dates['train']['end'])]\n",
    "    val_df = data_df.loc[(data_df['timestamp'] >= split_dates['val']['start']) & (data_df['timestamp'] < split_dates['val']['end'])]\n",
    "    test_df = data_df.loc[(data_df['timestamp'] >= split_dates['test']['start']) & (data_df['timestamp'] < split_dates['test']['end'])]\n",
    "\n",
    "    train_df = train_df[features + ['time_of_day', 'weekday']]\n",
    "    val_df = val_df[features + ['time_of_day', 'weekday']]\n",
    "    test_df = test_df[features + ['time_of_day', 'weekday']]\n",
    "\n",
    "    train_df['carrier'] = train_df['carrier'].astype(\"category\").cat.codes\n",
    "    val_df['carrier'] = val_df['carrier'].astype(\"category\").cat.codes\n",
    "    test_df['carrier'] = test_df['carrier'].astype(\"category\").cat.codes\n",
    "\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5231af2d",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8af8e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tech in technologies:\n",
    "    tech_dir = os.path.join(folder_dir, tech)\n",
    "    os.makedirs(tech_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3f0230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop\n",
    "for technology in technologies:\n",
    "\n",
    "    save_dir = os.path.join(folder_dir, technology)\n",
    "    if verbose: print(technology)\n",
    "    \n",
    "    # Train val test split\n",
    "    tech_df = original_df.copy(deep=True)\n",
    "    train_df, val_df, test_df = train_val_test_split(tech_df, technology, features, split_dates)\n",
    "    if verbose: print('Train, val, test split done')\n",
    "    if verbose: print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "    if verbose: print('Normalizing target variable')\n",
    "    train_df['DL Throughput'] = np.log1p(train_df['DL Throughput'])\n",
    "    val_df['DL Throughput'] = np.log1p(val_df['DL Throughput'])\n",
    "    test_df['DL Throughput'] = np.log1p(test_df['DL Throughput'])\n",
    "\n",
    "    ## Keep normalization values\n",
    "    thpt_train_mean = train_df['DL Throughput'].dropna().mean()\n",
    "    thpt_train_std = train_df['DL Throughput'].dropna().std()\n",
    "\n",
    "    m = thpt_train_mean\n",
    "    s = thpt_train_std\n",
    "    train_df['DL Throughput'] = (train_df['DL Throughput'] - m) / s\n",
    "    val_df['DL Throughput'] = (val_df['DL Throughput'] - m) / s\n",
    "    test_df['DL Throughput'] = (test_df['DL Throughput'] - m) / s\n",
    "    if verbose: print('Throughput normalized')\n",
    "\n",
    "    # Convert pandas to numpy vectors: X_train, X_val and X_test\n",
    "    X_train = train_df[features].values.astype('float32')\n",
    "    y_train = train_df['DL Throughput'].values.astype('float32').reshape(-1, 1)\n",
    "    X_val = val_df[features].values.astype('float32')\n",
    "    y_val = val_df['DL Throughput'].values.astype('float32').reshape(-1, 1)\n",
    "    X_test = test_df[features].values.astype('float32')\n",
    "    y_test = test_df['DL Throughput'].values.astype('float32').reshape(-1, 1)\n",
    "\n",
    "    total_samples = len(y_train) + len(y_val) + len(y_test)\n",
    "    train_size = X_train.shape[0]\n",
    "    d = X_train.shape[1]\n",
    "    if verbose: print('Training arrays computed')\n",
    "    if verbose: print('Train/val/test proportions:', round(len(y_train) / total_samples,2), '/', \n",
    "                                        round(len(y_val) / total_samples,2), '/',\n",
    "                                        round(len(y_test) / total_samples,2))\n",
    "\n",
    "    # Saving datasets\n",
    "    np.save(os.path.join(save_dir, \"X_train.npy\"), X_train)\n",
    "    np.save(os.path.join(save_dir, \"X_val.npy\"), X_val)\n",
    "    np.save(os.path.join(save_dir, \"X_test.npy\"), X_test)\n",
    "    np.save(os.path.join(save_dir, \"y_train.npy\"), y_train)\n",
    "    np.save(os.path.join(save_dir, \"y_val.npy\"), y_val)\n",
    "    np.save(os.path.join(save_dir, \"y_test.npy\"), y_test)\n",
    "    np.save(os.path.join(save_dir, \"m.npy\"), thpt_train_mean)\n",
    "    np.save(os.path.join(save_dir, \"s.npy\"), thpt_train_std)\n",
    "    np.save(os.path.join(save_dir, \"features_lst.npy\"), np.array(features))\n",
    "    if verbose: print('Datasets saved')\n",
    "\n",
    "    if verbose: print('XGBoost model starting')\n",
    "\n",
    "    # XGBoost model definition\n",
    "    xgb_model = XGBRegressor(n_estimators=N_ESTIMATORS, max_depth=6, learning_rate=LR_GB, \n",
    "                        eval_metric=[\"rmse\", \"mae\"], early_stopping_rounds = PATIENCE_GB,\n",
    "                        random_state=RANDOM_SEED)\n",
    "    if verbose: print('XGBoost model defined')\n",
    "\n",
    "    # XGBRegressor \n",
    "    xgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_val, y_val)],  \n",
    "        verbose=True,\n",
    "    )\n",
    "    if verbose: print('XGBoost model trained')\n",
    "    if verbose: print(xgb_model.best_iteration)\n",
    "\n",
    "    xgb_model.save_model(os.path.join(save_dir, \"xgb_model.json\"))\n",
    "    if verbose: print('XGBoost model saved')\n",
    "\n",
    "    # XGBoost SHAP\n",
    "    if verbose: print('XGBoost SHAP starting')\n",
    "    \n",
    "    X_shap = X_test.copy()\n",
    "    if verbose: print('Shap values reset')\n",
    "    \n",
    "    explainer = shap.TreeExplainer(xgb_model, X_shap, feature_names=features)\n",
    "    shap_values = explainer(X_shap)\n",
    "\n",
    "    if verbose: print('XGBoost SHAP ended')\n",
    "\n",
    "    # Saving shap\n",
    "    with open(os.path.join(save_dir, \"xgb_shap.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(shap_values, f)\n",
    "    ax = shap.plots.beeswarm(shap_values, max_display=20, show=False)\n",
    "    fig = ax.get_figure() \n",
    "    fig.savefig(os.path.join(save_dir, \"xgboost_shap_beeswarm.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    plt.close()\n",
    "    if verbose: print('XGBoost shap saved')\n",
    "    if verbose: print('-------------------------------------------------------------')\n",
    "    \n",
    "    del xgb_model\n",
    "    del explainer, shap_values, X_shap\n",
    "\n",
    "    if verbose: print('NGBoost model starting')\n",
    "    ngb_model = NGBRegressor(n_estimators=N_ESTIMATORS, learning_rate=LR_GB,\n",
    "                    early_stopping_rounds=PATIENCE_GB,random_state=RANDOM_SEED)\n",
    "    ngb_model.fit(X_train, y_train,\n",
    "              X_val=X_val,\n",
    "              Y_val=y_val);\n",
    "\n",
    "    if verbose: print('NGBoost model trained')\n",
    "\n",
    "    with open(os.path.join(save_dir, \"ngboost_model.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(ngb_model, f)\n",
    "\n",
    "    if verbose: print('NGBoost model saved')\n",
    "\n",
    "    # NGBoost SHAP\n",
    "    if verbose: print('NGBoost SHAP starting')\n",
    "    \n",
    "    X_shap = X_test.copy()\n",
    "    if verbose: print('Shap values reset')\n",
    "\n",
    "    # NGBoost SHAP - mean\n",
    "    explainer_mean = shap.TreeExplainer(ngb_model, model_output=0, feature_names=features)\n",
    "    shap_values_ng_mean = explainer_mean(X_shap)\n",
    "\n",
    "    if verbose: print('NGBoost mean SHAP ended')\n",
    "    \n",
    "    # Saving shap\n",
    "    with open(os.path.join(save_dir, \"ngb_mean_shap.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(shap_values_ng_mean, f)\n",
    "    ax = shap.plots.beeswarm(shap_values_ng_mean, max_display=20, show=False)\n",
    "    fig = ax.get_figure() \n",
    "    fig.savefig(os.path.join(save_dir, \"ngboost_mean_shap_beeswarm.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    plt.close()\n",
    "    if verbose: print('NGBoost mean shap saved')\n",
    "\n",
    "    # NGBoost SHAP - std\n",
    "    X_shap = X_test.copy()\n",
    "\n",
    "    explainer_std = shap.TreeExplainer(ngb_model, model_output=1, feature_names=features) \n",
    "    shap_values_ng_std = explainer_std(X_shap)\n",
    "\n",
    "    if verbose: print('NGBoost std SHAP ended')\n",
    "\n",
    "    # Saving shap\n",
    "    with open(os.path.join(save_dir, \"ngb_std_shap.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(shap_values_ng_std, f)\n",
    "    ax = shap.plots.beeswarm(shap_values_ng_std, max_display=20, show=False)\n",
    "    fig = ax.get_figure() \n",
    "    fig.savefig(os.path.join(save_dir, \"ngboost_std_shap_beeswarm.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    plt.close()\n",
    "    if verbose: print('NGBoost mean shap saved')\n",
    "\n",
    "    # Make sure we re-initialize before going to the next round\n",
    "    del ngb_model\n",
    "    del explainer_mean, explainer_std, shap_values_ng_mean, shap_values_ng_std, X_shap\n",
    "    del X_train, y_train, X_val, y_val, X_test, y_test\n",
    "    del train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7fcac2",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from xgboost import XGBRegressor\n",
    "from ngboost import NGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176a2f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dir = '/v1_6_data_and_models/'\n",
    "\n",
    "dir_dict = {'4G' : folder_dir+'4G/',\n",
    "            '5G_NSA' : folder_dir+'5G_NSA/',\n",
    "            '5G_SA' : folder_dir+'5G_SA/'\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544c741e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javialbert/miniconda3/envs/py312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model_data_dict = {}\n",
    "\n",
    "for tech in dir_dict:\n",
    "    model_data_dict[tech] = {}\n",
    "    X_train = np.load(os.path.join(dir_dict[tech], \"X_train.npy\"))\n",
    "    y_train = np.load(os.path.join(dir_dict[tech], \"y_train.npy\"))\n",
    "    X_val = np.load(os.path.join(dir_dict[tech], \"X_val.npy\"))\n",
    "    y_val = np.load(os.path.join(dir_dict[tech], \"y_val.npy\"))\n",
    "    y_test = np.load(os.path.join(dir_dict[tech], \"y_test.npy\"))\n",
    "    X_test = np.load(os.path.join(dir_dict[tech], \"X_test.npy\"))\n",
    "    m = np.load(os.path.join(dir_dict[tech], \"m.npy\"))\n",
    "    s = np.load(os.path.join(dir_dict[tech], \"s.npy\"))\n",
    "    X_shap = X_test.copy()\n",
    "    features = np.load(os.path.join(dir_dict[tech], \"features_lst.npy\"), allow_pickle=True) \n",
    "    shap_features = features\n",
    "    \n",
    "    with open(os.path.join(dir_dict[tech], \"xgb_shap.pkl\"), \"rb\") as f:\n",
    "        xgb_shap = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join(dir_dict[tech], \"ngb_mean_shap.pkl\"), \"rb\") as f:\n",
    "        ngboost_shap_mean = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join(dir_dict[tech], \"ngb_std_shap.pkl\"), \"rb\") as f:\n",
    "        ngboost_shap_std = pickle.load(f)\n",
    "\n",
    "    train_size = X_train.shape[0]\n",
    "    d = X_train.shape[1]\n",
    "\n",
    "    xgb_model = XGBRegressor()\n",
    "    xgb_model.load_model(os.path.join(dir_dict[tech], \"xgb_model.json\"))\n",
    "\n",
    "    with open(os.path.join(dir_dict[tech], \"ngboost_model.pkl\"), \"rb\") as f:\n",
    "        ngb_model = pickle.load(f)\n",
    "\n",
    "    model_data_dict[tech]['xgb_model'] = xgb_model\n",
    "    model_data_dict[tech]['ngb_model'] = ngb_model\n",
    "    model_data_dict[tech]['X_train'] = X_train\n",
    "    model_data_dict[tech]['y_train'] = y_train\n",
    "    model_data_dict[tech]['X_val'] = X_val\n",
    "    model_data_dict[tech]['y_val'] = y_val\n",
    "    model_data_dict[tech]['X_test'] = X_test\n",
    "    model_data_dict[tech]['y_test'] = y_test\n",
    "    model_data_dict[tech]['X_shap'] = X_shap\n",
    "    model_data_dict[tech]['features'] = features\n",
    "    model_data_dict[tech]['m'] = float(m)\n",
    "    model_data_dict[tech]['s'] = float(s)\n",
    "    model_data_dict[tech]['shap_features'] = shap_features\n",
    "    model_data_dict[tech]['xgb_shap'] = xgb_shap\n",
    "    model_data_dict[tech]['ngb_shap_mean'] = ngboost_shap_mean\n",
    "    model_data_dict[tech]['ngb_shap_std'] = ngboost_shap_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d320364",
   "metadata": {},
   "source": [
    "## Accuracy Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6936390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.special import erf\n",
    "\n",
    "def _phi(z):  # standard normal pdf\n",
    "    return (1.0 / np.sqrt(2 * np.pi)) * np.exp(-0.5 * z * z)\n",
    "\n",
    "def _Phi(z):  # standard normal cdf\n",
    "    return 0.5 * (1.0 + erf(z / np.sqrt(2.0)))\n",
    "\n",
    "def crps_gaussian(mu, sigma, y, eps=1e-12):\n",
    "    sigma = np.maximum(np.asarray(sigma, dtype=float), eps).flatten()\n",
    "    mu = np.asarray(mu, dtype=float).flatten()\n",
    "    y = np.asarray(y, dtype=float).flatten()\n",
    "    z = (y - mu) / sigma\n",
    "    return sigma * (z * (2.0 * _Phi(z) - 1.0) + 2.0 * _phi(z) - 1.0 / np.sqrt(np.pi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ef6ce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {}\n",
    "for tech in model_data_dict:\n",
    "    metrics_dict[tech] = {}\n",
    "    xgb_model = model_data_dict[tech]['xgb_model']\n",
    "    ngb_model = model_data_dict[tech]['ngb_model']\n",
    "    X_test = model_data_dict[tech]['X_test']\n",
    "    y_test = model_data_dict[tech]['y_test']\n",
    "    m = model_data_dict[tech]['m']\n",
    "    s = model_data_dict[tech]['s']\n",
    "\n",
    "    y_test_nat = np.expm1(y_test * s + m)\n",
    "\n",
    "    # ---- XGB metrics ----\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    y_pred_xgb_nat = np.expm1(y_pred_xgb * s + m)\n",
    "    metrics_dict[tech]['xgb_model'] = {\n",
    "        'mae': mean_absolute_error(y_test, y_pred_xgb),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_test, y_pred_xgb)),\n",
    "        'r2': r2_score(y_test, y_pred_xgb),\n",
    "        'crps': mean_absolute_error(y_test, y_pred_xgb),\n",
    "        'mae_nats' : mean_absolute_error(y_test_nat, y_pred_xgb_nat),\n",
    "        'rmse_nats' : np.sqrt(mean_squared_error(y_test_nat, y_pred_xgb_nat))\n",
    "    }\n",
    "\n",
    "    # ---- NGB metrics ----\n",
    "    y_pred_dist_ng = ngb_model.pred_dist(X_test)  # predictive distribution\n",
    "    mu = y_pred_dist_ng.mean().flatten()\n",
    "    sigma = y_pred_dist_ng.std().flatten()\n",
    "    y_pred_mean_ng = y_pred_dist_ng.mean()  # mean prediction\n",
    "    y_pred_mean_nat_ng = np.expm1(y_pred_mean_ng * s + m)\n",
    "    metrics_dict[tech]['ngb_model'] = {\n",
    "        'mae': mean_absolute_error(y_test, y_pred_mean_ng),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_test, y_pred_mean_ng)),\n",
    "        'r2': r2_score(y_test, y_pred_mean_ng),\n",
    "        'crps': np.mean(crps_gaussian(mu, sigma, y_test)),\n",
    "        'mae_nats' : mean_absolute_error(y_test_nat, y_pred_mean_nat_ng),\n",
    "        'rmse_nats' : np.sqrt(mean_squared_error(y_test_nat, y_pred_mean_nat_ng))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955eb112",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame.from_dict(\n",
    "    {(tech, model): metrics_dict[tech][model] \n",
    "     for tech in metrics_dict.keys() \n",
    "     for model in metrics_dict[tech].keys()},\n",
    "    orient='index'\n",
    ")\n",
    "\n",
    "df_metrics = df_metrics.round(4) # apply only to this DataFrame\n",
    "print(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7229b9",
   "metadata": {},
   "source": [
    "## Confidence Interval Plots Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "896c1705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.lines as mlines\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "CI_PERCENTILE = 0.95\n",
    "z = scipy.stats.norm.ppf(0.5 + CI_PERCENTILE/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe7ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación\n",
    "y_test_pred_xgb = xgb_model.predict(X_test)\n",
    "y_test_pred_dist = ngb_model.pred_dist(X_test)\n",
    "y_test_pred_m = y_test_pred_dist.mean()\n",
    "y_test_pred_s = y_test_pred_dist.std()\n",
    "\n",
    "y_test_nat = np.expm1(y_test * s + m)\n",
    "\n",
    "y_test_pred_xgb_nat = np.expm1(y_test_pred_xgb * s + m)\n",
    "y_test_pred_m_log = y_test_pred_dist.mean()\n",
    "y_test_pred_s_log = y_test_pred_dist.std()\n",
    "y_test_pred_m_nat = np.expm1(y_test_pred_m_log * s + m)\n",
    "y_test_pred_m_nat_lb = np.expm1((y_test_pred_m_log - z * y_test_pred_s_log) * s + m)\n",
    "y_test_pred_m_nat_ub = np.expm1((y_test_pred_m_log + z * y_test_pred_s_log) * s + m)\n",
    "\n",
    "# Visualización de ICs para DISPLAY_N muestras\n",
    "DISPLAY_N = 50\n",
    "idx = np.random.choice(len(X_test), DISPLAY_N, replace=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "for i, j in enumerate(idx):\n",
    "    xgb_pred_i = y_test_pred_xgb_nat[j]\n",
    "    mu_i = y_test_pred_m_log[j]\n",
    "    std_i = y_test_pred_s_log[j]\n",
    "    y_true = y_test_nat[j]\n",
    "\n",
    "    lower_i = y_test_pred_m_nat_lb[j]\n",
    "    upper_i = y_test_pred_m_nat_ub[j]\n",
    "    inside = lower_i <= y_true <= upper_i\n",
    "    color = 'green' if inside else 'red'\n",
    "\n",
    "    ax.plot([i, i], [lower_i, upper_i], color=color, linewidth=8, alpha = 0.4)\n",
    "    ax.plot(i, y_true, marker='x', color='black', label='Measured DL Throughput' if i == 0 else \"\")\n",
    "    ax.plot(i, xgb_pred_i, marker='o', color='orange', alpha = 1, label='XGBoost Estimate' if i == 0 else \"\")\n",
    "    ax.plot(i, np.expm1(mu_i * s + m), marker='o', color='blue', alpha = 1, label=r'NGBoost $\\mu$ Estimate' if i == 0 else \"\")\n",
    "\n",
    "# Existing legend from plot\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "# Custom handle for NGBoost σ (red–green line)\n",
    "sigma_handle = mlines.Line2D([], [], color='red', linewidth=6, alpha=0.4)\n",
    "sigma_handle2 = mlines.Line2D([], [], color='green', linewidth=6, alpha=0.4)\n",
    "\n",
    "# Put both on top of each other (hack: treat as a single entry by grouping)\n",
    "handles.append((sigma_handle, sigma_handle2))\n",
    "labels.append(r'NGBoost $\\sigma$ Estimate')\n",
    "\n",
    "ax.legend(handles, labels, fontsize=16, handler_map={tuple: HandlerTuple(ndivide=None)})\n",
    "\n",
    "# Larger labels\n",
    "ax.set_xlabel(\"Sample index\", fontsize=20)\n",
    "ax.set_ylabel(\"Throughput (Kbps)\", fontsize=20)\n",
    "\n",
    "# Larger tick labels\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=14)\n",
    "\n",
    "ax.set_ylim([100, 10**6])\n",
    "ax.grid(True)\n",
    "ax.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(os.path.join(folder_dir, 'Fig1.5G_SA_CIs.pdf'), \n",
    "            format='pdf', bbox_inches='tight', pad_inches=0.05)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495eae7e",
   "metadata": {},
   "source": [
    "## Calibration Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72757c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphas_coverage(model, X, y, alphas=np.linspace(0.01, 0.99, 25)):\n",
    "\n",
    "    dist_test = model.pred_dist(X)\n",
    "    means = dist_test.mean().flatten()\n",
    "    stds  = dist_test.std().flatten()\n",
    "\n",
    "    coverage = []\n",
    "    interval_length = []\n",
    "    for alpha in alphas:\n",
    "        z = scipy.stats.norm.ppf(0.5 + alpha/2)\n",
    "        lower = means - z * stds\n",
    "        upper = means + z * stds\n",
    "        inside = (y.squeeze() >= lower) & (y.squeeze() <= upper)\n",
    "        coverage.append(np.mean(inside))\n",
    "        interval_length.append((upper - lower).mean())\n",
    "\n",
    "    diff = np.array(coverage) - alphas\n",
    "    ece = np.trapz(np.abs(diff), alphas)\n",
    "\n",
    "    return alphas, coverage, ece, interval_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e403145",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "for tech in model_data_dict:\n",
    "\n",
    "    alphas, coverage, ece, interval_length = get_alphas_coverage(model_data_dict[tech]['ngb_model'], \n",
    "                                    model_data_dict[tech]['X_test'], \n",
    "                                    model_data_dict[tech]['y_test'])\n",
    "    ax.plot(alphas, coverage, markersize = 5, marker='X',\n",
    "                alpha=0.8, linestyle = '--', label=tech + ' NGB ' + f' (C-AUC = {ece:.3f})')\n",
    "\n",
    "ax.plot([0,1], [0,1], linestyle='--', color='gray', label='Ideal')\n",
    "\n",
    "ax.set_xlabel(\"Nominal Confidence Level (1 - α)\", fontsize=20)\n",
    "ax.set_ylabel(\"Empirical Coverage\", fontsize=20)\n",
    "\n",
    "# Larger tick labels\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=14)\n",
    "\n",
    "ax.grid(True)\n",
    "ax.legend(fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(os.path.join(folder_dir, 'Fig2.Cal_curves.pdf'), \n",
    "            format='pdf', bbox_inches='tight', pad_inches=0.05)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01881061",
   "metadata": {},
   "source": [
    "## SHAP heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2afe355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define color palettes for each group\n",
    "from matplotlib import cm\n",
    "from itertools import cycle\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9758899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order by feature groups\n",
    "features_order = ['Frequency Band', 'Carrier',\n",
    "                  'RSRP', 'RSRQ', 'SINR', 'Timing Advance',\n",
    "                  'Latency', 'Jitter', 'Packet Loss', 'DL TTFB',\n",
    "                  'Time of the Day', 'Day of the Week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da330765",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_name_lst = [item for item in model_data_dict['5G_NSA'].keys() if '_shap' in item and not 'X_shap' in item] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69364c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "col_to_shap_name_dict = {}\n",
    "for key in dir_dict.keys():\n",
    "    cols.append(key + '_XGB')\n",
    "    col_to_shap_name_dict['XGB'] = 'xgb_shap'\n",
    "    cols.append(key + '_NGB_MEAN')\n",
    "    col_to_shap_name_dict['NGB_MEAN'] = 'ngb_shap_mean'\n",
    "    cols.append(key + '_NGB_STD')\n",
    "    col_to_shap_name_dict['NGB_STD'] = 'ngb_shap_std'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21c16c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = cols, \n",
    "                  index = model_data_dict['5G_NSA']['shap_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a369821",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tech in model_data_dict.keys():\n",
    "    for col_name in cols:\n",
    "        if not col_name.startswith(tech):\n",
    "            continue\n",
    "        shap_name = col_to_shap_name_dict[col_name[len(tech)+1:]]\n",
    "        df[col_name] = np.abs(model_data_dict[tech][shap_name].values).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8f6b429",
   "metadata": {},
   "outputs": [],
   "source": [
    "tech = '5G_SA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637d599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = ['Frequency Band', 'Carrier']\n",
    "radio = ['RSRP', 'RSRQ', 'SINR', 'Timing Advance']\n",
    "e2e = ['Latency', 'Jitter', 'Packet Loss', 'DL TTFB']\n",
    "context = ['Time of the Day', 'Day of the Week']\n",
    "\n",
    "features_order = radio + e2e + context + deployment\n",
    "\n",
    "color_map = {}\n",
    "\n",
    "color_map.update({f: c for f, c in zip(deployment, cm.Greys(np.linspace(0.4, 0.7, len(deployment))))})\n",
    "color_map.update({f: c for f, c in zip(radio, cm.Blues(np.linspace(0.4, 0.7, len(radio))))})\n",
    "color_map.update({f: c for f, c in zip(e2e, cm.Reds(np.linspace(0.4, 0.7, len(e2e))))})\n",
    "color_map.update({f: c for f, c in zip(context, cm.Greens(np.linspace(0.4, 0.7, len(context))))})\n",
    "\n",
    "df_vals = df.astype(float)\n",
    "\n",
    "first_model = df_vals.columns[0]\n",
    "top_features = df_vals[first_model].nlargest(20).index\n",
    "\n",
    "df_top = df_vals.loc[top_features]\n",
    "df_top = df_top.loc[features_order]\n",
    "\n",
    "df_norm = df_top.div(df_vals.sum(axis=0), axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4.5), dpi=150)\n",
    "\n",
    "bottom_vals = np.zeros(len(df_norm.columns))\n",
    "for feature in df_norm.index:\n",
    "    ax.bar(df_norm.columns, df_norm.loc[feature],\n",
    "           bottom=bottom_vals,\n",
    "           label=feature,\n",
    "           color=color_map[feature],   # same mapping as before\n",
    "           edgecolor='black', linewidth=0.5,\n",
    "           width=0.5)\n",
    "    bottom_vals += df_norm.loc[feature].values\n",
    "\n",
    "ax.axvline(x=2.5, color='black', linestyle='-', linewidth=1)  # between 3rd and 4th ticks\n",
    "ax.axvline(x=5.5, color='black', linestyle='-', linewidth=1)  # between 6th and 7th ticks\n",
    "\n",
    "ax.axvspan(-0.5, 0.5, facecolor='none', edgecolor='orange',\n",
    "           hatch='///', alpha=0.6, linewidth=0.0, zorder=0)\n",
    "\n",
    "ax.axvspan(0.5, 2.5, facecolor='none', edgecolor='blue',\n",
    "           hatch='///', alpha=0.6, linewidth=0.0, zorder=0)\n",
    "\n",
    "ax.axvspan(2.5, 3.5, facecolor='none', edgecolor='orange',\n",
    "           hatch='///', alpha=0.6, linewidth=0.0, zorder=0)\n",
    "\n",
    "ax.axvspan(3.5, 5.5, facecolor='none', edgecolor='blue',\n",
    "           hatch='///', alpha=0.6, linewidth=0.0, zorder=0)\n",
    "\n",
    "ax.axvspan(5.5, 6.5, facecolor='none', edgecolor='orange',\n",
    "           hatch='///', alpha=0.6, linewidth=0.0, zorder=0)\n",
    "\n",
    "ax.axvspan(6.5, 8.5, facecolor='none', edgecolor='blue',\n",
    "           hatch='///', alpha=0.6, linewidth=0.0, zorder=0)\n",
    "\n",
    "ax.set_ylabel(\"Normalized Importance\")\n",
    "ax.legend(title=\"Feature\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "tick_labels = (['XGB', r'NGB $\\mu$', r'NGB $\\sigma$'] * 3)  # repeat 3 times\n",
    "ax.set_xticks(np.arange(len(df_norm.columns)))\n",
    "ax.set_xticklabels(tick_labels, rotation=0, fontsize=9)\n",
    "\n",
    "ax.tick_params(axis='x', which='major', pad=5)  \n",
    "\n",
    "group_labels = ['4G', '5G_NSA', '5G_SA']\n",
    "group_sizes  = [3, 3, 3]  \n",
    "\n",
    "pos = np.arange(len(df_norm.columns))\n",
    "start = 0\n",
    "for label, size in zip(group_labels, group_sizes):\n",
    "    center = start + (size-1)/2\n",
    "    ax.text(center, -0.08, label, ha='center', va='top',\n",
    "            transform=ax.get_xaxis_transform(),\n",
    "            fontsize=11)\n",
    "    start += size\n",
    "\n",
    "h_xgb = mpatches.Patch(facecolor='none', edgecolor='orange',\n",
    "                       hatch='///', label='XGBoost', linewidth=0.0)\n",
    "h_ngb = mpatches.Patch(facecolor='none', edgecolor='blue',\n",
    "                       hatch='///', label='NGBoost', linewidth=0.0)\n",
    "\n",
    "feat_legend = ax.legend(title=\"Feature\", bbox_to_anchor=(1, 1), loc='upper left')\n",
    "\n",
    "model_legend = ax.legend(handles=[h_xgb, h_ngb], title=\"Model encoding\",\n",
    "                         bbox_to_anchor=(1, 0.2), loc='upper left', frameon=True)\n",
    "ax.add_artist(feat_legend)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(os.path.join(folder_dir, 'Fig3.Feature_importance.pdf'), \n",
    "            format='pdf', bbox_inches='tight', pad_inches=0.05, bbox_extra_artists = (feat_legend, model_legend, ax))\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e861ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(8, 3), sharey=True, constrained_layout=True)\n",
    "\n",
    "fontsize = 12\n",
    "\n",
    "X_shap = model_data_dict[tech]['X_shap']\n",
    "xgb_shap = model_data_dict[tech]['xgb_shap']\n",
    "ngboost_shap_mean = model_data_dict[tech]['ngb_shap_mean']\n",
    "\n",
    "for idx, feature_name in enumerate(['Frequency Band', 'RSRP', 'DL TTFB']):\n",
    "\n",
    "    feature_idx_shap = list(shap_features).index(feature_name)\n",
    "    feature_idx_x = list(features).index(feature_name)\n",
    "\n",
    "    axs[idx].axhline(0, color='gray', linestyle='--', alpha=0.7, label='Zero SHAP')\n",
    "\n",
    "    axs[idx].scatter(X_shap[:, feature_idx_x], xgb_shap.values[:, feature_idx_shap],\n",
    "                color='blue', alpha=0.5, label='XGBoost')\n",
    "    axs[idx].scatter(X_shap[:, feature_idx_x], ngboost_shap_mean.values[:, feature_idx_shap],\n",
    "                color='orange', alpha=0.5, label=r'NGBoost $\\mu$')\n",
    "\n",
    "    # axis labels\n",
    "    if feature_name == 'RSRP':\n",
    "        axs[idx].set_xlabel(feature_name + ' (dBm)', fontsize=fontsize)\n",
    "    elif feature_name == 'Frequency Band':\n",
    "        axs[idx].set_xlabel(feature_name + ' (MHz)', fontsize=fontsize)\n",
    "    elif feature_name == 'DL TTFB':\n",
    "        axs[idx].set_xlabel(feature_name + ' (ms)', fontsize=fontsize)\n",
    "\n",
    "    axs[idx].set_ylim([-2.5, 1.5])\n",
    "\n",
    "    if feature_name == 'DL TTFB':\n",
    "        axs[idx].set_xscale('log')\n",
    "        axs[idx].set_xlim([75, 1.5e3])\n",
    "\n",
    "    axs[idx].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # tick labels larger\n",
    "    axs[idx].tick_params(axis='both', labelsize=fontsize)\n",
    "\n",
    "# y-axis label\n",
    "axs[0].set_ylabel('SHAP value', fontsize=fontsize)\n",
    "\n",
    "# legend with larger font\n",
    "axs[2].legend(fontsize=fontsize - 2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(os.path.join(folder_dir, 'Fig4.SHAP_spread.pdf'),\n",
    "            format='pdf', bbox_inches='tight', pad_inches=0.05)\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c8e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm.loc[e2e].sum(axis = 0) / df_norm.loc[radio].sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc61ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm.loc['RSRQ'] / df_norm.loc[radio].sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae1156",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm.loc['RSRP'] / df_norm.loc[radio].sum(axis = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
